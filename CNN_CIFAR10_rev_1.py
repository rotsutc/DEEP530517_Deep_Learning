import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, top_k_accuracy_score
import numpy as np
import pandas as pd

# 1. T·∫£i d·ªØ li·ªáu CIFAR-10
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()

# Chu·∫©n h√≥a d·ªØ li·ªáu
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# One-hot encoding cho nh√£n
num_classes = 10
y_train_cat = keras.utils.to_categorical(y_train, num_classes)
y_test_cat = keras.utils.to_categorical(y_test, num_classes)

# Danh s√°ch t√™n l·ªõp trong CIFAR-10
class_names = ["airplane", "automobile", "bird", "cat", "deer",
               "dog", "frog", "horse", "ship", "truck"]

# 2. X√¢y d·ª±ng m√¥ h√¨nh CNN v·ªõi 5 hidden layers
model = models.Sequential([
    layers.Input(shape=(32,32,3)),

    # Hidden layer 1
    layers.Conv2D(32, (3,3), activation='relu', padding="same"),
    layers.MaxPooling2D((2,2)),

    # Hidden layer 2
    layers.Conv2D(64, (3,3), activation='relu', padding="same"),
    layers.MaxPooling2D((2,2)),

    # Hidden layer 3
    layers.Conv2D(128, (3,3), activation='relu', padding="same"),
    layers.MaxPooling2D((2,2)),

    # Hidden layer 4
    layers.Conv2D(128, (3,3), activation='relu', padding="same"),

    # Flatten sang vector
    layers.Flatten(),

    # Hidden layer 5 (fully connected)
    layers.Dense(128, activation='relu'),

    # Output layer
    layers.Dense(num_classes, activation='softmax')
])


# 3. Compile m√¥ h√¨nh
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
# T√≥m t·∫Øt ki·∫øn tr√∫c
model.summary()

# 4. Hu·∫•n luy·ªán m√¥ h√¨nh
history = model.fit(x_train, y_train_cat,
                    epochs=5,
                    batch_size=128,
                    validation_split=0.2,
                    verbose=1)

# 5. ƒê√°nh gi√° tr√™n test set
test_loss, test_acc = model.evaluate(x_test, y_test_cat, verbose=2)
print("\nüéØ Test accuracy:", test_acc)


def show_maximized():
    mng = plt.get_current_fig_manager()
    try:
        mng.window.state('zoomed')  # Windows
    except AttributeError:
        try:
            mng.window.showMaximized()  # Qt backend
        except AttributeError:
            try:
                mng.resize(*mng.window.maxsize())  # TkAgg / Linux
            except Exception:
                pass
    plt.show()
    
# 6. V·∫Ω bi·ªÉu ƒë·ªì Loss v√† Accuracy
plt.figure(figsize=(12,5))

# Loss
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title("Loss per Epoch")
plt.xlabel("Epoch")         # ‚úÖ th√™m nh√£n tr·ª•c X
plt.ylabel("Loss")          # ‚úÖ th√™m nh√£n tr·ª•c Y

# Accuracy
plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.legend()
plt.title("Accuracy per Epoch")
plt.xlabel("Epoch")         # ‚úÖ th√™m nh√£n tr·ª•c X
plt.ylabel("Accuracy")      # ‚úÖ th√™m nh√£n tr·ª•c Y

show_maximized() 


# 7. D·ª± ƒëo√°n th·ª≠ 1 ·∫£nh t·ª´ test set
idx = np.random.randint(0, len(x_test))  # ch·ªçn ng·∫´u nhi√™n 1 ·∫£nh
img = x_test[idx]
true_label = y_test[idx][0]

# D·ª± ƒëo√°n
pred_probs = model.predict(img.reshape(1,32,32,3))
pred_label = np.argmax(pred_probs)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
plt.imshow(img)
plt.axis("off")
plt.title(f"True: {class_names[true_label]}, Pred: {class_names[pred_label]}")
show_maximized() 

# 8. Hi·ªÉn th·ªã 10 ·∫£nh test ƒë·∫ßu ti√™n k√®m d·ª± ƒëo√°n
plt.figure(figsize=(15, 6))

for i in range(10):
    img = x_test[i]
    true_label = y_test[i][0]
    
    # D·ª± ƒëo√°n
    pred_probs = model.predict(img.reshape(1,32,32,3), verbose=0)
    pred_label = np.argmax(pred_probs)
    
    # V·∫Ω ·∫£nh
    plt.subplot(2, 5, i+1)
    plt.imshow(img)
    plt.axis("off")
    color = "green" if pred_label == true_label else "red"
    plt.title(f"T:{class_names[true_label]}\nP:{class_names[pred_label]}", color=color)

plt.suptitle("10 ·∫£nh test ƒë·∫ßu ti√™n: T = True, P = Predicted", fontsize=14)
show_maximized() 

# 9. T√≠nh nh√£n d·ª± ƒëo√°n cho to√†n b·ªô test set
y_pred_probs = model.predict(x_test, verbose=0)
y_pred = np.argmax(y_pred_probs, axis=1)
y_true = y_test.flatten()

# 10. V·∫Ω confusion matrix
cm = confusion_matrix(y_true, y_pred)

# V·∫Ω confusion matrix
fig, ax = plt.subplots(figsize=(10, 8))
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
disp.plot(cmap="Blues", xticks_rotation=45, values_format="d", ax=ax)

# Th√™m ti√™u ƒë·ªÅ v√† nh√£n tr·ª•c
ax.set_title("Confusion Matrix - CIFAR-10", fontsize=14)
ax.set_xlabel("Predicted label", fontsize=12)
ax.set_ylabel("True label", fontsize=12)

# ƒêi·ªÅu ch·ªânh layout 
plt.subplots_adjust(
    left=0.125,
    bottom=0.145,
    right=0.9,
    top=0.88,
    wspace=0.2,
    hspace=0.2
)
show_maximized() 

# 11. Classification report
report = classification_report(y_true, y_pred, target_names=class_names)
print("üìä Classification Report:\n")
print(report)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, top_k_accuracy_score

# 12. T√≠nh c√°c metrics t·ªïng qu√°t
acc = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred, average="macro")   # macro: trung b√¨nh ƒë·ªÅu gi·ªØa c√°c l·ªõp
rec = recall_score(y_true, y_pred, average="macro")
f1 = f1_score(y_true, y_pred, average="macro")

# F1@Top5: d√πng top-k accuracy ƒë·ªÉ xem nh√£n th·∫≠t c√≥ n·∫±m trong top-5 d·ª± ƒëo√°n hay kh√¥ng
# ·ªû ƒë√¢y precision@5 = recall@5 n√™n F1@5 c≈©ng b·∫±ng gi√° tr·ªã ƒë√≥
precision_at5 = top_k_accuracy_score(y_true, y_pred_probs, k=5, labels=range(num_classes))
recall_at5 = precision_at5
f1_at5 = 2 * (precision_at5 * recall_at5) / (precision_at5 + recall_at5)

print("\nüìä Metrics (macro-average):")
print(f"Accuracy   : {acc:.4f}")
print(f"Precision  : {prec:.4f}")
print(f"Recall     : {rec:.4f}")
print(f"F1-score   : {f1:.4f}")
print(f"F1@Top-5   : {f1_at5:.4f}")

import pandas as pd

# 13. L·∫•y classification report d·∫°ng dict
report_dict = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)

# Chuy·ªÉn th√†nh DataFrame (ch·ªâ l·∫•y 10 l·ªõp)
df_report = pd.DataFrame(report_dict).transpose().iloc[:10, :3]  # ch·ªâ l·∫•y precision, recall, f1-score
df_report = df_report.round(3)  # l√†m g·ªçn s·ªë th·∫≠p ph√¢n

print("\nüìä B·∫£ng per-class Precision / Recall / F1:\n")
print(df_report)

# 14. V·∫Ω bi·ªÉu ƒë·ªì c·ªôt
df_report.plot(kind="bar", figsize=(12,6))
plt.title("Per-class Precision / Recall / F1 - CIFAR-10", fontsize=14)
plt.xlabel("Class")
plt.ylabel("Score")
plt.xticks(range(len(class_names)), class_names, rotation=45)
plt.ylim(0,1)
plt.legend(loc="lower right")
plt.tight_layout()
show_maximized()
